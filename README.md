# transformer-scratch
Creating transformers based LLM from scratch

## ğŸ¯ Project Overview

This repository contains a comprehensive implementation of transformer-based language models built from scratch using PyTorch. The goal is to provide a clear, educational journey through the core components of modern LLMs while maintaining clean, well-documented code that others can learn from.

## ğŸ“ Project Structure

```
transformer-scratch/
â”œâ”€â”€ src/transformer/              # Core transformer components
â”‚   â”œâ”€â”€ scaled_dot_product_attention.py  # Core attention mechanism  
â”‚   â”œâ”€â”€ multihead_attention.py    # Multi-head attention wrapper
â”‚   â”œâ”€â”€ layers.py                 # Transformer layers & blocks
â”‚   â”œâ”€â”€ embeddings.py             # Position & token embeddings
â”‚   â”œâ”€â”€ models.py                 # Complete model architectures
â”‚   â””â”€â”€ utils.py                  # Utility functions
â”œâ”€â”€ examples/                     # Usage examples & tutorials
â”œâ”€â”€ tests/                        # Unit tests
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for exploration
â””â”€â”€ PROJECT_PLAN.md              # Detailed roadmap
```

## ğŸš€ Quick Start

1. **Clone the repository**
   ```bash
   git clone https://github.com/hetkotak/transformer-scratch.git
   cd transformer-scratch
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run examples**
   ```bash
   python examples/01_scaled_dot_product_attention.py
   python examples/02_multihead_attention.py
   ```

## ğŸ“š Current Progress

- [x] Project structure setup
- [x] Scaled dot-product attention implementation
- [ ] Multi-head attention implementation
- [ ] Transformer layers
- [ ] Complete GPT model

## ğŸ› ï¸ Development

Run tests:
```bash
python -m pytest tests/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
